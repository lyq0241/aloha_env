I0710 15:55:33.390051 135460982499136 02_finetune_new_observation_action.py:59] Loading pre-trained model...
Fetching 8 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 24018.92it/s]
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
W0710 15:55:37.143767 135460982499136 block_transformer.py:295] Using old attention computation from released December models.
I0710 15:55:43.790688 135460982499136 checkpointer.py:164] Restoring item from /home/yunqiliu/.cache/huggingface/hub/models--rail-berkeley--octo-small/snapshots/03d88976c54a58e10480d2043a8c762b35bc2611/270000/default.
I0710 15:55:44.433576 135460982499136 checkpointer.py:166] Finished restoring checkpoint from /home/yunqiliu/.cache/huggingface/hub/models--rail-berkeley--octo-small/snapshots/03d88976c54a58e10480d2043a8c762b35bc2611/270000/default.
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
I0710 15:55:45.945927 135460982499136 02_finetune_new_observation_action.py:68] Loading finetuning dataset...
I0710 15:55:45.951794 135460982499136 dataset_info.py:578] Load dataset info from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
I0710 15:55:46.846988 135460982499136 logging_logger.py:49] Constructing tf.data.Dataset aloha_sim_cube_scripted_dataset for split all, from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
This model is trained with a window size of 2, predicting 7 dimensional actions 4 steps into the future.
Observations and tasks conform to the following spec:
Observations: {
    image_primary: ('batch', 'history_window', 256, 256, 3),
    image_wrist: ('batch', 'history_window', 128, 128, 3),
}
Tasks: {
    image_primary: ('batch', 256, 256, 3),
    image_wrist: ('batch', 128, 128, 3),
    language_instruction: {
        attention_mask: ('batch', 16),
        input_ids: ('batch', 16),
    },
}
At inference, you may pass in any subset of these observation and task keys, with a history window up to 2 timesteps.
I0710 15:55:47.548912 135460982499136 function_wrappers.py:112] Sampling uniformly across keys: ['language_instruction']
I0710 15:55:47.740691 135460982499136 data_utils.py:113] Loading existing dataset statistics from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0/dataset_statistics_6e9a30171a12844cc8c8c309ca19f52e9b09ba3dce8b5c743bb122a40baba07c.json.
I0710 15:55:47.777811 135460982499136 logging_logger.py:49] Constructing tf.data.Dataset aloha_sim_cube_scripted_dataset for split train[:95%], from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
I0710 15:55:47.832562 135460982499136 function_wrappers.py:112] Sampling uniformly across keys: ['language_instruction']
I0710 15:55:56.140849 135460982499136 02_finetune_new_observation_action.py:128] Updating model for new observation & action space...
I0710 15:55:57.435651 135460982499136 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
W0710 15:55:57.634769 135460982499136 block_transformer.py:410] Prefix groups:
W0710 15:55:57.635154 135460982499136 block_transformer.py:412] PrefixGroup(name=task_language, shape=(1, 16, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:55:57.635291 135460982499136 block_transformer.py:418] Timestep groups:
W0710 15:55:57.635422 135460982499136 block_transformer.py:420] TimestepGroup(name=obs_primary, shape=(1, 1, 256, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:55:57.635539 135460982499136 block_transformer.py:420] TimestepGroup(name=obs_proprio, shape=(1, 1, 14, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:55:57.635650 135460982499136 block_transformer.py:420] TimestepGroup(name=readout_action, shape=(1, 1, 1, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    readout_action: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
[3m                                                                        Attention Mask                                                                        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m                                [22m┃[1m task_language (16 tokens) [22m┃[1m t=0 obs_primary (256 tokens)  [22m┃[1m t=0 obs_proprio (14 tokens)  [22m┃[1m t=0 readout_action (1 tokens)  [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ task_language (16 tokens)      │ x                         │ x                             │ x                            │ x                              │
├────────────────────────────────┼───────────────────────────┼───────────────────────────────┼──────────────────────────────┼────────────────────────────────┤
│ t=0 obs_primary (256 tokens)   │                           │ x                             │ x                            │ x                              │
├────────────────────────────────┼───────────────────────────┼───────────────────────────────┼──────────────────────────────┼────────────────────────────────┤
│ t=0 obs_proprio (14 tokens)    │                           │ x                             │ x                            │ x                              │
├────────────────────────────────┼───────────────────────────┼───────────────────────────────┼──────────────────────────────┼────────────────────────────────┤
│ t=0 readout_action (1 tokens)  │                           │                               │                              │ x                              │
└────────────────────────────────┴───────────────────────────┴───────────────────────────────┴──────────────────────────────┴────────────────────────────────┘
W0710 15:55:57.642319 135460982499136 block_transformer.py:295] Using old attention computation from released December models.
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[3m                                                                        OctoModule Summary                                                                        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m path                            [22m┃[1m module                [22m┃[1m inputs                          [22m┃[1m outputs                         [22m┃[1m params                           [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│                                 │ OctoModule            │ - image_primary:                │ - obs:                          │                                  │
│                                 │                       │ uint8[1,1,256,256,3]            │     mask: bool[1,1,270]         │                                  │
│                                 │                       │   pad_mask_dict:                │     tokens:                     │                                  │
│                                 │                       │     image_primary: bool[1,1]    │ float32[1,1,270,384]            │                                  │
│                                 │                       │     proprio: bool[1,1]          │   obs_primary:                  │                                  │
│                                 │                       │     timestep: bool[1,1]         │     mask: bool[1,1,256]         │                                  │
│                                 │                       │   proprio: float32[1,1,14]      │     tokens:                     │                                  │
│                                 │                       │   task_completed: bool[1,1,50]  │ float32[1,1,256,384]            │                                  │
│                                 │                       │   timestep: int32[1,1]          │   obs_proprio:                  │                                  │
│                                 │                       │   timestep_pad_mask: bool[1,1]  │     mask: bool[1,1,14]          │                                  │
│                                 │                       │ - language_instruction:         │     tokens: float32[1,1,14,384] │                                  │
│                                 │                       │     attention_mask: int32[1,16] │   readout_action:               │                                  │
│                                 │                       │     input_ids: int32[1,16]      │     mask: float32[1,1,1]        │                                  │
│                                 │                       │   pad_mask_dict:                │     tokens: float32[1,1,1,384]  │                                  │
│                                 │                       │     language_instruction:       │   task:                         │                                  │
│                                 │                       │ bool[1]                         │     mask: bool[1,16]            │                                  │
│                                 │                       │ - bool[1,1]                     │     tokens: float32[1,16,384]   │                                  │
│                                 │                       │ - train: False                  │   task_language:                │                                  │
│                                 │                       │   verbose: True                 │     mask: bool[1,16]            │                                  │
│                                 │                       │                                 │     tokens: float32[1,16,384]   │                                  │
│                                 │                       │                                 │ - action: float32[1,1,50,14]    │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer                │ OctoTransformer       │ - image_primary:                │ obs:                            │ obs_primary_pos_embedding:       │
│                                 │                       │ uint8[1,1,256,256,3]            │   mask: bool[1,1,270]           │ float32[1,10,256,384]            │
│                                 │                       │   pad_mask_dict:                │   tokens: float32[1,1,270,384]  │ obs_proprio_pos_embedding:       │
│                                 │                       │     image_primary: bool[1,1]    │ obs_primary:                    │ float32[1,10,14,384]             │
│                                 │                       │     proprio: bool[1,1]          │   mask: bool[1,1,256]           │ readout_action_pos_embedding:    │
│                                 │                       │     timestep: bool[1,1]         │   tokens: float32[1,1,256,384]  │ float32[1,10,1,384]              │
│                                 │                       │   proprio: float32[1,1,14]      │ obs_proprio:                    │ task_language_pos_embedding:     │
│                                 │                       │   task_completed: bool[1,1,50]  │   mask: bool[1,1,14]            │ float32[1,16,384]                │
│                                 │                       │   timestep: int32[1,1]          │   tokens: float32[1,1,14,384]   │                                  │
│                                 │                       │   timestep_pad_mask: bool[1,1]  │ readout_action:                 │ [1m1,046,784 (4.2 MB)[22m               │
│                                 │                       │ - language_instruction:         │   mask: float32[1,1,1]          │                                  │
│                                 │                       │     attention_mask: int32[1,16] │   tokens: float32[1,1,1,384]    │                                  │
│                                 │                       │     input_ids: int32[1,16]      │ task:                           │                                  │
│                                 │                       │   pad_mask_dict:                │   mask: bool[1,16]              │                                  │
│                                 │                       │     language_instruction:       │   tokens: float32[1,16,384]     │                                  │
│                                 │                       │ bool[1]                         │ task_language:                  │                                  │
│                                 │                       │ - bool[1,1]                     │   mask: bool[1,16]              │                                  │
│                                 │                       │ - train: False                  │   tokens: float32[1,16,384]     │                                  │
│                                 │                       │   verbose: True                 │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ encoder                         │ FlaxT5Stack           │ attention_mask: int32[1,1]      │ attentions: None                │                                  │
│                                 │                       │ deterministic: True             │ cross_attentions: None          │                                  │
│                                 │                       │ input_ids: int32[1,1]           │ hidden_states: None             │                                  │
│                                 │                       │ output_attentions: False        │ last_hidden_state:              │                                  │
│                                 │                       │ output_hidden_states: False     │ float32[1,1,768]                │                                  │
│                                 │                       │ return_dict: True               │ past_key_values: None           │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ shared                          │ Embed                 │ int32[1,1]                      │ float32[1,1,768]                │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ encoder/dropout                 │ Dropout               │ - float32[1,1,768]              │ float32[1,1,768]                │                                  │
│                                 │                       │ - deterministic: True           │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ encoder/block                   │ FlaxT5BlockCollection │ - float32[1,1,768]              │ attentions: None                │                                  │
│                                 │                       │ - attention_mask: int32[1,1]    │ cross_attentions: None          │                                  │
│                                 │                       │   deterministic: True           │ hidden_states: None             │                                  │
│                                 │                       │   encoder_attention_mask: None  │ last_hidden_state:              │                                  │
│                                 │                       │   encoder_hidden_states: None   │ float32[1,1,768]                │                                  │
│                                 │                       │   init_cache: False             │ past_key_values: None           │                                  │
│                                 │                       │   output_attentions: False      │                                 │                                  │
│                                 │                       │   output_hidden_states: False   │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ encoder/final_layer_norm        │ FlaxT5LayerNorm       │ float32[1,1,768]                │ float32[1,1,768]                │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/task_tokenize… │ LanguageTokenizer     │ - image_primary:                │ mask: bool[1,16]                │ [1m109,628,544 (438.5 MB)[22m           │
│                                 │                       │ uint8[1,1,256,256,3]            │ tokens: float32[1,16,768]       │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     image_primary: bool[1,1]    │                                 │                                  │
│                                 │                       │     proprio: bool[1,1]          │                                 │                                  │
│                                 │                       │     timestep: bool[1,1]         │                                 │                                  │
│                                 │                       │   proprio: float32[1,1,14]      │                                 │                                  │
│                                 │                       │   task_completed: bool[1,1,50]  │                                 │                                  │
│                                 │                       │   timestep: int32[1,1]          │                                 │                                  │
│                                 │                       │   timestep_pad_mask: bool[1,1]  │                                 │                                  │
│                                 │                       │ - language_instruction:         │                                 │                                  │
│                                 │                       │     attention_mask: int32[1,16] │                                 │                                  │
│                                 │                       │     input_ids: int32[1,16]      │                                 │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     language_instruction:       │                                 │                                  │
│                                 │                       │ bool[1]                         │                                 │                                  │
│                                 │                       │ - train: False                  │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/task_language… │ Dense                 │ float32[1,16,768]               │ float32[1,16,384]               │ bias: float32[384]               │
│                                 │                       │                                 │                                 │ kernel: float32[768,384]         │
│                                 │                       │                                 │                                 │                                  │
│                                 │                       │                                 │                                 │ [1m295,296 (1.2 MB)[22m                 │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/observation_t… │ ImageTokenizer        │ - image_primary:                │ mask: bool[1,1,256]             │ [1m1,058,048 (4.2 MB)[22m               │
│                                 │                       │ uint8[1,1,256,256,3]            │ tokens: float32[1,1,256,512]    │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     image_primary: bool[1,1]    │                                 │                                  │
│                                 │                       │     proprio: bool[1,1]          │                                 │                                  │
│                                 │                       │     timestep: bool[1,1]         │                                 │                                  │
│                                 │                       │   proprio: float32[1,1,14]      │                                 │                                  │
│                                 │                       │   task_completed: bool[1,1,50]  │                                 │                                  │
│                                 │                       │   timestep: int32[1,1]          │                                 │                                  │
│                                 │                       │   timestep_pad_mask: bool[1,1]  │                                 │                                  │
│                                 │                       │ - language_instruction:         │                                 │                                  │
│                                 │                       │     attention_mask: int32[1,16] │                                 │                                  │
│                                 │                       │     input_ids: int32[1,16]      │                                 │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     language_instruction:       │                                 │                                  │
│                                 │                       │ bool[1]                         │                                 │                                  │
│                                 │                       │ - train: False                  │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/obs_primary_p… │ Dense                 │ float32[1,1,256,512]            │ float32[1,1,256,384]            │ bias: float32[384]               │
│                                 │                       │                                 │                                 │ kernel: float32[512,384]         │
│                                 │                       │                                 │                                 │                                  │
│                                 │                       │                                 │                                 │ [1m196,992 (788.0 KB)[22m               │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/observation_t… │ LowdimObsTokenizer    │ - image_primary:                │ mask: float32[1,1,14]           │                                  │
│                                 │                       │ uint8[1,1,256,256,3]            │ tokens: float32[1,1,14,1]       │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     image_primary: bool[1,1]    │                                 │                                  │
│                                 │                       │     proprio: bool[1,1]          │                                 │                                  │
│                                 │                       │     timestep: bool[1,1]         │                                 │                                  │
│                                 │                       │   proprio: float32[1,1,14]      │                                 │                                  │
│                                 │                       │   task_completed: bool[1,1,50]  │                                 │                                  │
│                                 │                       │   timestep: int32[1,1]          │                                 │                                  │
│                                 │                       │   timestep_pad_mask: bool[1,1]  │                                 │                                  │
│                                 │                       │ - language_instruction:         │                                 │                                  │
│                                 │                       │     attention_mask: int32[1,16] │                                 │                                  │
│                                 │                       │     input_ids: int32[1,16]      │                                 │                                  │
│                                 │                       │   pad_mask_dict:                │                                 │                                  │
│                                 │                       │     language_instruction:       │                                 │                                  │
│                                 │                       │ bool[1]                         │                                 │                                  │
│                                 │                       │ - train: False                  │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/obs_proprio_p… │ Dense                 │ float32[1,1,14,1]               │ float32[1,1,14,384]             │ bias: float32[384]               │
│                                 │                       │                                 │                                 │ kernel: float32[1,384]           │
│                                 │                       │                                 │                                 │                                  │
│                                 │                       │                                 │                                 │ [1m768 (3.1 KB)[22m                     │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ octo_transformer/BlockTransfor… │ BlockTransformer      │ - - attention_rules:            │ - - attention_rules:            │ [1m21,294,336 (85.2 MB)[22m             │
│                                 │                       │       task_*:                   │       task_*:                   │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │     mask: bool[1,16]            │     mask: bool[1,16]            │                                  │
│                                 │                       │     name: task_language         │     name: task_language         │                                  │
│                                 │                       │     tokens: float32[1,16,384]   │     tokens: float32[1,16,384]   │                                  │
│                                 │                       │ - - attention_rules:            │ - - attention_rules:            │                                  │
│                                 │                       │       obs_*:                    │       obs_*:                    │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │       task_*:                   │       task_*:                   │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │     mask: bool[1,1,256]         │     mask: bool[1,1,256]         │                                  │
│                                 │                       │     name: obs_primary           │     name: obs_primary           │                                  │
│                                 │                       │     tokens:                     │     tokens:                     │                                  │
│                                 │                       │ float32[1,1,256,384]            │ float32[1,1,256,384]            │                                  │
│                                 │                       │   - attention_rules:            │   - attention_rules:            │                                  │
│                                 │                       │       obs_*:                    │       obs_*:                    │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │       task_*:                   │       task_*:                   │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │     mask: bool[1,1,14]          │     mask: bool[1,1,14]          │                                  │
│                                 │                       │     name: obs_proprio           │     name: obs_proprio           │                                  │
│                                 │                       │     tokens: float32[1,1,14,384] │     tokens: float32[1,1,14,384] │                                  │
│                                 │                       │   - attention_rules:            │   - attention_rules:            │                                  │
│                                 │                       │       obs_*:                    │       obs_*:                    │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │       readout_action:           │       readout_action:           │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │       task_*:                   │       task_*:                   │                                  │
│                                 │                       │ <AttentionRule.CAUSAL:          │ <AttentionRule.CAUSAL:          │                                  │
│                                 │                       │ other.timestep <=               │ other.timestep <=               │                                  │
│                                 │                       │ self.timestep>                  │ self.timestep>                  │                                  │
│                                 │                       │     mask: float32[1,1,1]        │     mask: float32[1,1,1]        │                                  │
│                                 │                       │     name: readout_action        │     name: readout_action        │                                  │
│                                 │                       │     tokens: float32[1,1,1,384]  │     tokens: float32[1,1,1,384]  │                                  │
│                                 │                       │ - train: False                  │                                 │                                  │
│                                 │                       │   verbose: True                 │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ heads_action                    │ L1ActionHead          │ - obs:                          │ float32[1,1,50,14]              │                                  │
│                                 │                       │     mask: bool[1,1,270]         │                                 │                                  │
│                                 │                       │     tokens:                     │                                 │                                  │
│                                 │                       │ float32[1,1,270,384]            │                                 │                                  │
│                                 │                       │   obs_primary:                  │                                 │                                  │
│                                 │                       │     mask: bool[1,1,256]         │                                 │                                  │
│                                 │                       │     tokens:                     │                                 │                                  │
│                                 │                       │ float32[1,1,256,384]            │                                 │                                  │
│                                 │                       │   obs_proprio:                  │                                 │                                  │
│                                 │                       │     mask: bool[1,1,14]          │                                 │                                  │
│                                 │                       │     tokens: float32[1,1,14,384] │                                 │                                  │
│                                 │                       │   readout_action:               │                                 │                                  │
│                                 │                       │     mask: float32[1,1,1]        │                                 │                                  │
│                                 │                       │     tokens: float32[1,1,1,384]  │                                 │                                  │
│                                 │                       │   task:                         │                                 │                                  │
│                                 │                       │     mask: bool[1,16]            │                                 │                                  │
│                                 │                       │     tokens: float32[1,16,384]   │                                 │                                  │
│                                 │                       │   task_language:                │                                 │                                  │
│                                 │                       │     mask: bool[1,16]            │                                 │                                  │
│                                 │                       │     tokens: float32[1,16,384]   │                                 │                                  │
│                                 │                       │ - train: False                  │                                 │                                  │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ heads_action/map_head           │ MAPHead               │ - mask: float32[1,1,1]          │ float32[1,1,1,384]              │ probe: float32[1,1,384]          │
│                                 │                       │   tokens: float32[1,1,1,384]    │                                 │                                  │
│                                 │                       │ - train: False                  │                                 │ [1m1,774,080 (7.1 MB)[22m               │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│ heads_action/mean_proj          │ Dense                 │ float32[1,1,384]                │ float32[1,1,700]                │ bias: float32[700]               │
│                                 │                       │                                 │                                 │ kernel: float32[384,700]         │
│                                 │                       │                                 │                                 │                                  │
│                                 │                       │                                 │                                 │ [1m269,500 (1.1 MB)[22m                 │
├─────────────────────────────────┼───────────────────────┼─────────────────────────────────┼─────────────────────────────────┼──────────────────────────────────┤
│[1m                                 [22m│[1m                       [22m│[1m                                 [22m│[1m                           Total [22m│[1m 135,564,348 (542.3 MB)           [22m│
└─────────────────────────────────┴───────────────────────┴─────────────────────────────────┴─────────────────────────────────┴──────────────────────────────────┘
[1m                                                                                                                                                                  
[1m                                                             Total Parameters: 135,564,348 (542.3 MB)                                                             
I0710 15:56:01.664123 135460982499136 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
W0710 15:56:01.789132 135460982499136 block_transformer.py:295] Using old attention computation from released December models.
I0710 15:56:38.636192 135460982499136 train_utils.py:405] ########## Parameters skipped during model loading: ##########
I0710 15:56:38.636617 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.LayerNorm_0.bias
I0710 15:56:38.636696 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.LayerNorm_0.scale
I0710 15:56:38.636744 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_0.bias
I0710 15:56:38.636790 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_0.kernel
I0710 15:56:38.636834 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_1.bias
I0710 15:56:38.636876 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_1.kernel
I0710 15:56:38.636921 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.key.bias
I0710 15:56:38.636965 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.key.kernel
I0710 15:56:38.637009 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.out.bias
I0710 15:56:38.637051 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.out.kernel
I0710 15:56:38.637094 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.query.bias
I0710 15:56:38.637139 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.query.kernel
I0710 15:56:38.637183 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.value.bias
I0710 15:56:38.637225 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.value.kernel
I0710 15:56:38.637272 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.probe
I0710 15:56:38.637316 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.mean_proj.bias
I0710 15:56:38.637361 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.mean_proj.kernel
I0710 15:56:38.637402 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_pos_embedding
I0710 15:56:38.637446 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_projection.bias
I0710 15:56:38.637488 135460982499136 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_projection.kernel
I0710 15:56:38.653193 135460982499136 train_utils.py:250] Freezing parameters that include the following keys: ['*hf_model*'].
I0710 15:56:38.656830 135460982499136 train_utils.py:286] Num trainable params: 25,935,804.
I0710 15:56:38.656934 135460982499136 train_utils.py:287] Num frozen params: 109,628,544.
I0710 15:56:38.656989 135460982499136 train_utils.py:288] To see a detailed list of frozen params, set logging level to DEBUG.
I0710 15:56:39.094842 135460982499136 02_finetune_new_observation_action.py:186] Starting finetuning...
  0%|                                                                                                                    | 0/5000 [00:00<?, ?it/s]I0710 15:56:40.578148 135460982499136 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
W0710 15:56:40.787113 135460982499136 block_transformer.py:295] Using old attention computation from released December models.








































