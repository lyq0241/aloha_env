I0710 15:02:15.191736 125989617137472 02_finetune_new_observation_action.py:60] Loading pre-trained model...
Fetching 8 files: 100%|███████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 10037.22it/s]
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
W0710 15:02:19.616104 125989617137472 block_transformer.py:295] Using old attention computation from released December models.
I0710 15:02:26.396935 125989617137472 checkpointer.py:164] Restoring item from /home/yunqiliu/.cache/huggingface/hub/models--rail-berkeley--octo-small/snapshots/03d88976c54a58e10480d2043a8c762b35bc2611/270000/default.
I0710 15:02:27.050348 125989617137472 checkpointer.py:166] Finished restoring checkpoint from /home/yunqiliu/.cache/huggingface/hub/models--rail-berkeley--octo-small/snapshots/03d88976c54a58e10480d2043a8c762b35bc2611/270000/default.
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
I0710 15:02:28.617393 125989617137472 02_finetune_new_observation_action.py:69] Loading finetuning dataset...
I0710 15:02:28.623149 125989617137472 dataset_info.py:578] Load dataset info from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
I0710 15:02:28.706580 125989617137472 logging_logger.py:49] Constructing tf.data.Dataset aloha_sim_cube_scripted_dataset for split all, from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
This model is trained with a window size of 2, predicting 7 dimensional actions 4 steps into the future.
Observations and tasks conform to the following spec:
Observations: {
    image_primary: ('batch', 'history_window', 256, 256, 3),
    image_wrist: ('batch', 'history_window', 128, 128, 3),
}
Tasks: {
    image_primary: ('batch', 256, 256, 3),
    image_wrist: ('batch', 128, 128, 3),
    language_instruction: {
        attention_mask: ('batch', 16),
        input_ids: ('batch', 16),
    },
}
At inference, you may pass in any subset of these observation and task keys, with a history window up to 2 timesteps.
I0710 15:02:29.448291 125989617137472 function_wrappers.py:112] Sampling uniformly across keys: ['language_instruction']
I0710 15:02:29.674896 125989617137472 data_utils.py:113] Loading existing dataset statistics from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0/dataset_statistics_6e9a30171a12844cc8c8c309ca19f52e9b09ba3dce8b5c743bb122a40baba07c.json.
I0710 15:02:29.715564 125989617137472 logging_logger.py:49] Constructing tf.data.Dataset aloha_sim_cube_scripted_dataset for split train[:95%], from /home/yunqiliu/tensorflow_datasets/aloha_sim_cube_scripted_dataset/1.0.0
I0710 15:02:29.770481 125989617137472 function_wrappers.py:112] Sampling uniformly across keys: ['language_instruction']
I0710 15:02:35.869796 125989617137472 02_finetune_new_observation_action.py:129] Updating model for new observation & action space...
I0710 15:02:37.017084 125989617137472 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
[3m                                                        Attention Mask                                                         
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m                                [22m┃[1m task_language (16    [22m┃[1m t=0 obs_primary (256 [22m┃[1m t=0 obs_proprio (14   [22m┃[1m t=0 readout_action   [22m┃
┃[1m                                [22m┃[1m tokens)              [22m┃[1m tokens)              [22m┃[1m tokens)               [22m┃[1m (1 tokens)           [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩
│ task_language (16 tokens)      │ x                    │ x                    │ x                     │ x                    │
├────────────────────────────────┼──────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┤
│ t=0 obs_primary (256 tokens)   │                      │ x                    │ x                     │ x                    │
├────────────────────────────────┼──────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┤
│ t=0 obs_proprio (14 tokens)    │                      │ x                    │ x                     │ x                    │
├────────────────────────────────┼──────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┤
│ t=0 readout_action (1 tokens)  │                      │                      │                       │ x                    │
└────────────────────────────────┴──────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┘
W0710 15:02:37.152392 125989617137472 block_transformer.py:410] Prefix groups:
W0710 15:02:37.152678 125989617137472 block_transformer.py:412] PrefixGroup(name=task_language, shape=(1, 16, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:02:37.152774 125989617137472 block_transformer.py:418] Timestep groups:
W0710 15:02:37.152847 125989617137472 block_transformer.py:420] TimestepGroup(name=obs_primary, shape=(1, 1, 256, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:02:37.152932 125989617137472 block_transformer.py:420] TimestepGroup(name=obs_proprio, shape=(1, 1, 14, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:02:37.153012 125989617137472 block_transformer.py:420] TimestepGroup(name=readout_action, shape=(1, 1, 1, 384), attends_to={
    task_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    obs_*: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
    readout_action: <AttentionRule.CAUSAL: 'other.timestep <= self.timestep'>,
})
W0710 15:02:37.158056 125989617137472 block_transformer.py:295] Using old attention computation from released December models.
/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[3m                                                      OctoModule Summary                                                       
┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m path                    [22m┃[1m module                [22m┃[1m inputs                 [22m┃[1m outputs                 [22m┃[1m params                 [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩
│                         │ OctoModule            │ - image_primary:       │ - obs:                  │                        │
│                         │                       │ uint8[1,1,256,256,3]   │     mask: bool[1,1,270] │                        │
│                         │                       │   pad_mask_dict:       │     tokens:             │                        │
│                         │                       │     image_primary:     │ float32[1,1,270,384]    │                        │
│                         │                       │ bool[1,1]              │   obs_primary:          │                        │
│                         │                       │     proprio: bool[1,1] │     mask: bool[1,1,256] │                        │
│                         │                       │     timestep:          │     tokens:             │                        │
│                         │                       │ bool[1,1]              │ float32[1,1,256,384]    │                        │
│                         │                       │   proprio:             │   obs_proprio:          │                        │
│                         │                       │ float32[1,1,14]        │     mask: bool[1,1,14]  │                        │
│                         │                       │   task_completed:      │     tokens:             │                        │
│                         │                       │ bool[1,1,50]           │ float32[1,1,14,384]     │                        │
│                         │                       │   timestep: int32[1,1] │   readout_action:       │                        │
│                         │                       │   timestep_pad_mask:   │     mask:               │                        │
│                         │                       │ bool[1,1]              │ float32[1,1,1]          │                        │
│                         │                       │ -                      │     tokens:             │                        │
│                         │                       │ language_instruction:  │ float32[1,1,1,384]      │                        │
│                         │                       │     attention_mask:    │   task:                 │                        │
│                         │                       │ int32[1,16]            │     mask: bool[1,16]    │                        │
│                         │                       │     input_ids:         │     tokens:             │                        │
│                         │                       │ int32[1,16]            │ float32[1,16,384]       │                        │
│                         │                       │   pad_mask_dict:       │   task_language:        │                        │
│                         │                       │     language_instruct… │     mask: bool[1,16]    │                        │
│                         │                       │ bool[1]                │     tokens:             │                        │
│                         │                       │ - bool[1,1]            │ float32[1,16,384]       │                        │
│                         │                       │ - train: False         │ - action:               │                        │
│                         │                       │   verbose: True        │ float32[1,1,50,14]      │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer        │ OctoTransformer       │ - image_primary:       │ obs:                    │ obs_primary_pos_embed… │
│                         │                       │ uint8[1,1,256,256,3]   │   mask: bool[1,1,270]   │ float32[1,10,256,384]  │
│                         │                       │   pad_mask_dict:       │   tokens:               │ obs_proprio_pos_embed… │
│                         │                       │     image_primary:     │ float32[1,1,270,384]    │ float32[1,10,14,384]   │
│                         │                       │ bool[1,1]              │ obs_primary:            │ readout_action_pos_em… │
│                         │                       │     proprio: bool[1,1] │   mask: bool[1,1,256]   │ float32[1,10,1,384]    │
│                         │                       │     timestep:          │   tokens:               │ task_language_pos_emb… │
│                         │                       │ bool[1,1]              │ float32[1,1,256,384]    │ float32[1,16,384]      │
│                         │                       │   proprio:             │ obs_proprio:            │                        │
│                         │                       │ float32[1,1,14]        │   mask: bool[1,1,14]    │ [1m1,046,784 (4.2 MB)[22m     │
│                         │                       │   task_completed:      │   tokens:               │                        │
│                         │                       │ bool[1,1,50]           │ float32[1,1,14,384]     │                        │
│                         │                       │   timestep: int32[1,1] │ readout_action:         │                        │
│                         │                       │   timestep_pad_mask:   │   mask: float32[1,1,1]  │                        │
│                         │                       │ bool[1,1]              │   tokens:               │                        │
│                         │                       │ -                      │ float32[1,1,1,384]      │                        │
│                         │                       │ language_instruction:  │ task:                   │                        │
│                         │                       │     attention_mask:    │   mask: bool[1,16]      │                        │
│                         │                       │ int32[1,16]            │   tokens:               │                        │
│                         │                       │     input_ids:         │ float32[1,16,384]       │                        │
│                         │                       │ int32[1,16]            │ task_language:          │                        │
│                         │                       │   pad_mask_dict:       │   mask: bool[1,16]      │                        │
│                         │                       │     language_instruct… │   tokens:               │                        │
│                         │                       │ bool[1]                │ float32[1,16,384]       │                        │
│                         │                       │ - bool[1,1]            │                         │                        │
│                         │                       │ - train: False         │                         │                        │
│                         │                       │   verbose: True        │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ encoder                 │ FlaxT5Stack           │ attention_mask:        │ attentions: None        │                        │
│                         │                       │ int32[1,1]             │ cross_attentions: None  │                        │
│                         │                       │ deterministic: True    │ hidden_states: None     │                        │
│                         │                       │ input_ids: int32[1,1]  │ last_hidden_state:      │                        │
│                         │                       │ output_attentions:     │ float32[1,1,768]        │                        │
│                         │                       │ False                  │ past_key_values: None   │                        │
│                         │                       │ output_hidden_states:  │                         │                        │
│                         │                       │ False                  │                         │                        │
│                         │                       │ return_dict: True      │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ shared                  │ Embed                 │ int32[1,1]             │ float32[1,1,768]        │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ encoder/dropout         │ Dropout               │ - float32[1,1,768]     │ float32[1,1,768]        │                        │
│                         │                       │ - deterministic: True  │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ encoder/block           │ FlaxT5BlockCollection │ - float32[1,1,768]     │ attentions: None        │                        │
│                         │                       │ - attention_mask:      │ cross_attentions: None  │                        │
│                         │                       │ int32[1,1]             │ hidden_states: None     │                        │
│                         │                       │   deterministic: True  │ last_hidden_state:      │                        │
│                         │                       │   encoder_attention_m… │ float32[1,1,768]        │                        │
│                         │                       │ None                   │ past_key_values: None   │                        │
│                         │                       │   encoder_hidden_stat… │                         │                        │
│                         │                       │ None                   │                         │                        │
│                         │                       │   init_cache: False    │                         │                        │
│                         │                       │   output_attentions:   │                         │                        │
│                         │                       │ False                  │                         │                        │
│                         │                       │   output_hidden_state… │                         │                        │
│                         │                       │ False                  │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ encoder/final_layer_no… │ FlaxT5LayerNorm       │ float32[1,1,768]       │ float32[1,1,768]        │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/task_… │ LanguageTokenizer     │ - image_primary:       │ mask: bool[1,16]        │ [1m109,628,544 (438.5 MB)[22m │
│                         │                       │ uint8[1,1,256,256,3]   │ tokens:                 │                        │
│                         │                       │   pad_mask_dict:       │ float32[1,16,768]       │                        │
│                         │                       │     image_primary:     │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │     proprio: bool[1,1] │                         │                        │
│                         │                       │     timestep:          │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │   proprio:             │                         │                        │
│                         │                       │ float32[1,1,14]        │                         │                        │
│                         │                       │   task_completed:      │                         │                        │
│                         │                       │ bool[1,1,50]           │                         │                        │
│                         │                       │   timestep: int32[1,1] │                         │                        │
│                         │                       │   timestep_pad_mask:   │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │ -                      │                         │                        │
│                         │                       │ language_instruction:  │                         │                        │
│                         │                       │     attention_mask:    │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │     input_ids:         │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │   pad_mask_dict:       │                         │                        │
│                         │                       │     language_instruct… │                         │                        │
│                         │                       │ bool[1]                │                         │                        │
│                         │                       │ - train: False         │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/task_… │ Dense                 │ float32[1,16,768]      │ float32[1,16,384]       │ bias: float32[384]     │
│                         │                       │                        │                         │ kernel:                │
│                         │                       │                        │                         │ float32[768,384]       │
│                         │                       │                        │                         │                        │
│                         │                       │                        │                         │ [1m295,296 (1.2 MB)[22m       │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/obser… │ ImageTokenizer        │ - image_primary:       │ mask: bool[1,1,256]     │ [1m1,058,048 (4.2 MB)[22m     │
│                         │                       │ uint8[1,1,256,256,3]   │ tokens:                 │                        │
│                         │                       │   pad_mask_dict:       │ float32[1,1,256,512]    │                        │
│                         │                       │     image_primary:     │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │     proprio: bool[1,1] │                         │                        │
│                         │                       │     timestep:          │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │   proprio:             │                         │                        │
│                         │                       │ float32[1,1,14]        │                         │                        │
│                         │                       │   task_completed:      │                         │                        │
│                         │                       │ bool[1,1,50]           │                         │                        │
│                         │                       │   timestep: int32[1,1] │                         │                        │
│                         │                       │   timestep_pad_mask:   │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │ -                      │                         │                        │
│                         │                       │ language_instruction:  │                         │                        │
│                         │                       │     attention_mask:    │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │     input_ids:         │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │   pad_mask_dict:       │                         │                        │
│                         │                       │     language_instruct… │                         │                        │
│                         │                       │ bool[1]                │                         │                        │
│                         │                       │ - train: False         │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/obs_p… │ Dense                 │ float32[1,1,256,512]   │ float32[1,1,256,384]    │ bias: float32[384]     │
│                         │                       │                        │                         │ kernel:                │
│                         │                       │                        │                         │ float32[512,384]       │
│                         │                       │                        │                         │                        │
│                         │                       │                        │                         │ [1m196,992 (788.0 KB)[22m     │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/obser… │ LowdimObsTokenizer    │ - image_primary:       │ mask: float32[1,1,14]   │                        │
│                         │                       │ uint8[1,1,256,256,3]   │ tokens:                 │                        │
│                         │                       │   pad_mask_dict:       │ float32[1,1,14,1]       │                        │
│                         │                       │     image_primary:     │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │     proprio: bool[1,1] │                         │                        │
│                         │                       │     timestep:          │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │   proprio:             │                         │                        │
│                         │                       │ float32[1,1,14]        │                         │                        │
│                         │                       │   task_completed:      │                         │                        │
│                         │                       │ bool[1,1,50]           │                         │                        │
│                         │                       │   timestep: int32[1,1] │                         │                        │
│                         │                       │   timestep_pad_mask:   │                         │                        │
│                         │                       │ bool[1,1]              │                         │                        │
│                         │                       │ -                      │                         │                        │
│                         │                       │ language_instruction:  │                         │                        │
│                         │                       │     attention_mask:    │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │     input_ids:         │                         │                        │
│                         │                       │ int32[1,16]            │                         │                        │
│                         │                       │   pad_mask_dict:       │                         │                        │
│                         │                       │     language_instruct… │                         │                        │
│                         │                       │ bool[1]                │                         │                        │
│                         │                       │ - train: False         │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/obs_p… │ Dense                 │ float32[1,1,14,1]      │ float32[1,1,14,384]     │ bias: float32[384]     │
│                         │                       │                        │                         │ kernel: float32[1,384] │
│                         │                       │                        │                         │                        │
│                         │                       │                        │                         │ [1m768 (3.1 KB)[22m           │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ octo_transformer/Block… │ BlockTransformer      │ - - attention_rules:   │ - - attention_rules:    │ [1m21,294,336 (85.2 MB)[22m   │
│                         │                       │       task_*:          │       task_*:           │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │ other.timestep <=      │ other.timestep <=       │                        │
│                         │                       │ self.timestep>         │ self.timestep>          │                        │
│                         │                       │     mask: bool[1,16]   │     mask: bool[1,16]    │                        │
│                         │                       │     name:              │     name: task_language │                        │
│                         │                       │ task_language          │     tokens:             │                        │
│                         │                       │     tokens:            │ float32[1,16,384]       │                        │
│                         │                       │ float32[1,16,384]      │ - - attention_rules:    │                        │
│                         │                       │ - - attention_rules:   │       obs_*:            │                        │
│                         │                       │       obs_*:           │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ other.timestep <=       │                        │
│                         │                       │ other.timestep <=      │ self.timestep>          │                        │
│                         │                       │ self.timestep>         │       task_*:           │                        │
│                         │                       │       task_*:          │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ other.timestep <=       │                        │
│                         │                       │ other.timestep <=      │ self.timestep>          │                        │
│                         │                       │ self.timestep>         │     mask: bool[1,1,256] │                        │
│                         │                       │     mask:              │     name: obs_primary   │                        │
│                         │                       │ bool[1,1,256]          │     tokens:             │                        │
│                         │                       │     name: obs_primary  │ float32[1,1,256,384]    │                        │
│                         │                       │     tokens:            │   - attention_rules:    │                        │
│                         │                       │ float32[1,1,256,384]   │       obs_*:            │                        │
│                         │                       │   - attention_rules:   │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │       obs_*:           │ other.timestep <=       │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ self.timestep>          │                        │
│                         │                       │ other.timestep <=      │       task_*:           │                        │
│                         │                       │ self.timestep>         │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │       task_*:          │ other.timestep <=       │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ self.timestep>          │                        │
│                         │                       │ other.timestep <=      │     mask: bool[1,1,14]  │                        │
│                         │                       │ self.timestep>         │     name: obs_proprio   │                        │
│                         │                       │     mask: bool[1,1,14] │     tokens:             │                        │
│                         │                       │     name: obs_proprio  │ float32[1,1,14,384]     │                        │
│                         │                       │     tokens:            │   - attention_rules:    │                        │
│                         │                       │ float32[1,1,14,384]    │       obs_*:            │                        │
│                         │                       │   - attention_rules:   │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │       obs_*:           │ other.timestep <=       │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ self.timestep>          │                        │
│                         │                       │ other.timestep <=      │       readout_action:   │                        │
│                         │                       │ self.timestep>         │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │       readout_action:  │ other.timestep <=       │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ self.timestep>          │                        │
│                         │                       │ other.timestep <=      │       task_*:           │                        │
│                         │                       │ self.timestep>         │ <AttentionRule.CAUSAL:  │                        │
│                         │                       │       task_*:          │ other.timestep <=       │                        │
│                         │                       │ <AttentionRule.CAUSAL: │ self.timestep>          │                        │
│                         │                       │ other.timestep <=      │     mask:               │                        │
│                         │                       │ self.timestep>         │ float32[1,1,1]          │                        │
│                         │                       │     mask:              │     name:               │                        │
│                         │                       │ float32[1,1,1]         │ readout_action          │                        │
│                         │                       │     name:              │     tokens:             │                        │
│                         │                       │ readout_action         │ float32[1,1,1,384]      │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,1,1,384]     │                         │                        │
│                         │                       │ - train: False         │                         │                        │
│                         │                       │   verbose: True        │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ heads_action            │ L1ActionHead          │ - obs:                 │ float32[1,1,50,14]      │                        │
│                         │                       │     mask:              │                         │                        │
│                         │                       │ bool[1,1,270]          │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,1,270,384]   │                         │                        │
│                         │                       │   obs_primary:         │                         │                        │
│                         │                       │     mask:              │                         │                        │
│                         │                       │ bool[1,1,256]          │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,1,256,384]   │                         │                        │
│                         │                       │   obs_proprio:         │                         │                        │
│                         │                       │     mask: bool[1,1,14] │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,1,14,384]    │                         │                        │
│                         │                       │   readout_action:      │                         │                        │
│                         │                       │     mask:              │                         │                        │
│                         │                       │ float32[1,1,1]         │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,1,1,384]     │                         │                        │
│                         │                       │   task:                │                         │                        │
│                         │                       │     mask: bool[1,16]   │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,16,384]      │                         │                        │
│                         │                       │   task_language:       │                         │                        │
│                         │                       │     mask: bool[1,16]   │                         │                        │
│                         │                       │     tokens:            │                         │                        │
│                         │                       │ float32[1,16,384]      │                         │                        │
│                         │                       │ - train: False         │                         │                        │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ heads_action/map_head   │ MAPHead               │ - mask: float32[1,1,1] │ float32[1,1,1,384]      │ probe:                 │
│                         │                       │   tokens:              │                         │ float32[1,1,384]       │
│                         │                       │ float32[1,1,1,384]     │                         │                        │
│                         │                       │ - train: False         │                         │ [1m1,774,080 (7.1 MB)[22m     │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│ heads_action/mean_proj  │ Dense                 │ float32[1,1,384]       │ float32[1,1,700]        │ bias: float32[700]     │
│                         │                       │                        │                         │ kernel:                │
│                         │                       │                        │                         │ float32[384,700]       │
│                         │                       │                        │                         │                        │
│                         │                       │                        │                         │ [1m269,500 (1.1 MB)[22m       │
├─────────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┤
│[1m                         [22m│[1m                       [22m│[1m                        [22m│[1m                   Total [22m│[1m 135,564,348 (542.3 MB) [22m│
└─────────────────────────┴───────────────────────┴────────────────────────┴─────────────────────────┴────────────────────────┘
[1m                                                                                                                               
[1m                                           Total Parameters: 135,564,348 (542.3 MB)                                            
I0710 15:02:40.236520 125989617137472 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
W0710 15:02:40.322632 125989617137472 block_transformer.py:295] Using old attention computation from released December models.
I0710 15:03:17.193377 125989617137472 train_utils.py:405] ########## Parameters skipped during model loading: ##########
I0710 15:03:17.193945 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.LayerNorm_0.bias
I0710 15:03:17.194041 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.LayerNorm_0.scale
I0710 15:03:17.194105 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_0.bias
I0710 15:03:17.194166 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_0.kernel
I0710 15:03:17.194223 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_1.bias
I0710 15:03:17.194278 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MlpBlock_0.Dense_1.kernel
I0710 15:03:17.194337 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.key.bias
I0710 15:03:17.194392 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.key.kernel
I0710 15:03:17.194448 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.out.bias
I0710 15:03:17.194506 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.out.kernel
I0710 15:03:17.194562 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.query.bias
I0710 15:03:17.194618 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.query.kernel
I0710 15:03:17.194679 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.value.bias
I0710 15:03:17.194735 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.MultiHeadDotProductAttention_0.value.kernel
I0710 15:03:17.194792 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.map_head.probe
I0710 15:03:17.194848 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.mean_proj.bias
I0710 15:03:17.194901 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: heads_action.mean_proj.kernel
I0710 15:03:17.194954 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_pos_embedding
I0710 15:03:17.195016 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_projection.bias
I0710 15:03:17.195070 125989617137472 train_utils.py:407] Param missing in pre-trained model, skipping: octo_transformer.obs_proprio_projection.kernel
I0710 15:03:17.198917 125989617137472 train_utils.py:250] Freezing parameters that include the following keys: ['*hf_model*'].
I0710 15:03:17.203974 125989617137472 train_utils.py:286] Num trainable params: 25,935,804.
I0710 15:03:17.204103 125989617137472 train_utils.py:287] Num frozen params: 109,628,544.
I0710 15:03:17.204173 125989617137472 train_utils.py:288] To see a detailed list of frozen params, set logging level to DEBUG.
I0710 15:03:17.929952 125989617137472 02_finetune_new_observation_action.py:187] Starting finetuning...
  0%|                                                                                                 | 0/5000 [00:00<?, ?it/s]I0710 15:03:19.604328 125989617137472 tokenizers.py:124] No task inputs matching image_primary were found. Replacing with zero padding.
W0710 15:03:20.366376 125989617137472 block_transformer.py:295] Using old attention computation from released December models.













