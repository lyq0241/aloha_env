I0702 11:42:21.164527 138861603125056 03_eval_finetuned_copy.py:63] Loading finetuned model...

Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.29it/s]
Traceback (most recent call last):
  File "/home/yunqiliu/octo/examples/03_eval_finetuned_copy.py", line 226, in <module>
    app.run(main)
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
             ^^^^^^^^^^
  File "/home/yunqiliu/octo/examples/03_eval_finetuned_copy.py", line 72, in main
    tokenizer = AutoTokenizer.from_pretrained(vla_args.model_name_or_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 880, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2110, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2336, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 156, in __init__
    super().__init__(
  File "/home/yunqiliu/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 105, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.